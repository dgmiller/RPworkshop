# Linear Regression Example

### Objective

Learn how to develop your research using git and GitHub.

### To-Do

In this example, you will implement the following methods in `linear_regression.py` as well as tests for each method in `test_linear_regression.py`:

+ `compare_models()`
+ `simulate_data()`
+ `load_hospital_data()`
+ `prepare_data()`
+ `run_hospital_regression()`

**Let's Begin!**

This mini-paper will use two different packages to implement the same method (OLS). The results produced by both methods should be identical.

Suppose you want to run a regression or a series of regressions on a dataset about hospital charge data found online at data.gov/health. There are a number of questions we might want to ask this dataset. For this example, suppose you are interested in whether the amount that a hospital bills Medicare has an effect on the amount actually paid out by Medicare.

*Simulating Data*

A really good practice is to create a simulated dataset of the kind of data you will be working with. This has a number of advantages. First, the simulation is completely distinct from the actual dataset and avoids concerns about private or proprietary data. Second, it helps you think critically about the how the data were generated, also called the data generating process. This in turn helps you select an appropriate estimator because you know what the assumptions are about the data; you explicitly laid them out by simulating the dataset.

To begin, let's assume that the variable `x1` represents the average amount billed to Medicare. The name given for this variable in the hospital charges dataset is `average covered charges`. Suppose `x1` is uniformly distributed between 5,000 and 500,000. (Side note: it is apparent in the dataset that this not the correct choice of distribution. However, we can change the distribution at any time to see what happens to our estimates if the distribution isn't quite what we expected.) Now suppose there is another variable `x2` that we want to control for; it represents the number of hospital discharges associated with the given time period. Let it be poisson distributed with the mean at 15. 

Next we want to generate a vector of parameters `beta` that will represent the coefficients we want to estimate. The parameters should be normally distributed with mean 0 and stand deviation 2.5. In real world data sets, these parameters are never observed so it can be hard to know whether your method worked or not. While you can check all of your assumptions manually, it gives you more confidence if you can generate simulated data from a set of parameters and be able to check your parameter estimates against the simulated 'true' parameters.

Finally, the response variable `y` is generated by multiplying `X` by the vector `beta` and adding a noise term `epsilon` which is always assumed to be normally distributed with mean 0.

*Testing Methods on Simulated Data*

Now that we have some simulated data, we write a method to see if we can correctly estimate the true parameter values. We will compare the implementations in two Python packages: `statsmodels` and `sklearn`. Both methods should produce identical estimates of the parameters. (Hint: this might make for a good unit test.)

One nice thing about statistical estimators is that they come with some guarantees, assuming that all the assumptions are met sufficiently. This means that we can write some tests to make sure that the estimator satisfies certain requirements. For example, we can check to see that the estimated parameters are within some tolerance of the true paramter values. So long as these tests pass sufficiently often, it indicates that we have a good estimator.

*Preparing the Hospital Charges Dataset*

Now that we have validated our methods and know how to use each implementation in each package, we can start working with real data. The hospital charge data found at data.gov is not particularly large but it's not small either. It is not good to store large datasets on github. To get around this, it can sometimes be nice to sample observations from the large dataset and use that in continuing our model development and calibration. A sample from the hospital charges dataset is found in the file `hospital_charge_sample.csv`. We will use this smaller file when writing methods to prepare our data for analysis.

Initially loading the data may seem straitforward, but when the data is raw it may need to go through some processing before it can be prepared for regression. One good practice is to keep only the raw data and writing functions that 'clean' the dataset rather than changing it using your favorite spreadsheet program. This may not be possible for all applications but it is good practice when it is possible and avoids many accidental errors. It also keeps a log of how the data was changed so that you know what you're working with. This is why we will work with a sample dataset in this example.

Once the data is cleaned, meaning it follows standard conventions of a dataset (like Hadley Wickham's TIDY format) and can be easily loaded and worked with, then it is time to write a method that prepares that data to be inputed to the linear regression model. This may involve log-transforming some variables or creating new variables from linear combinations of others. In our case, just like when we simulated data, we will use the variables `average covered charges` and `total discharges` from the dataset. You can decide how this data needs to enter the regression.

*Run the Regression*

Finally, it's time to write a method that runs the regression for you. This method should involve the methods that clean the data and prepare the data for the regression. In this case, the output should come from the `statsmodels` package and should return the output table in text form as shown below. As soon as everything works and all tests pass, you can have complete confidence that when you use the full dataset, everything should work as expected. Once you have the simple model working, you can begin to experiment or add complexity.


```
                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:                      y   R-squared (uncentered):                   1.000
Model:                            OLS   Adj. R-squared (uncentered):              1.000
Method:                 Least Squares   F-statistic:                          7.712e+04
Date:                Mon, 16 Sep 2019   Prob (F-statistic):                   2.20e-172
Time:                        07:49:31   Log-Likelihood:                         -128.37
No. Observations:                 100   AIC:                                      276.7
Df Residuals:                      90   BIC:                                      302.8
Df Model:                          10                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
x1            -1.1245      0.019    -60.643      0.000      -1.161      -1.088
x2             2.5044      0.020    123.914      0.000       2.464       2.545
x3             1.6085      0.017     96.915      0.000       1.576       1.642
x4             6.2893      0.018    342.086      0.000       6.253       6.326
x5            -1.5883      0.020    -80.022      0.000      -1.628      -1.549
x6            -5.1080      0.018   -282.434      0.000      -5.144      -5.072
x7             2.7557      0.018    149.065      0.000       2.719       2.792
x8            -3.6592      0.019   -193.913      0.000      -3.697      -3.622
x9             0.5256      0.018     28.674      0.000       0.489       0.562
x10            0.5650      0.017     33.328      0.000       0.531       0.599
==============================================================================
Omnibus:                        1.103   Durbin-Watson:                   2.281
Prob(Omnibus):                  0.576   Jarque-Bera (JB):                0.695
Skew:                          -0.184   Prob(JB):                        0.706
Kurtosis:                       3.179   Cond. No.                         19.0
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
```


Congratulations! You have completed this exercise using a fully reproducible workflow. As you continue to learn more, you will find other ways to continue making your research more reproducible. This helps collaborators and other colleagues understand your work and gives you more confidence that your work is correct and accurate.
